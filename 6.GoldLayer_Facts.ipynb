{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3b53d2-75b3-4ca5-8e8a-626874691591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607b86bf-f63f-4c54-9c5b-17b96a18e2f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CDC column\n",
    "cdc_column = \"modifiedDate\"\n",
    "\n",
    "# Back-dated refresh\n",
    "backdated_refresh = \"\"\n",
    "\n",
    "# Catalog name\n",
    "catalog = \"flightsproject\"\n",
    "\n",
    "# Source object\n",
    "source_object = \"silver_bookings\"\n",
    "\n",
    "# Source schema\n",
    "source_schema = \"silver\"\n",
    "\n",
    "# Source fact table\n",
    "fact_table = f\"{catalog}.{source_schema}.{source_object}\"\n",
    "\n",
    "# Target schema\n",
    "target_schema = \"gold\"\n",
    "\n",
    "# Target object\n",
    "target_object = \"FactBookings\"\n",
    "\n",
    "# Fact key columns\n",
    "fact_key_cols = [\"DimPassengersKey\", \"DimFlightsKey\", \"DimAirportsKey\", \"booking_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd29f6d-5aa6-4826-8ce9-939dc7b0a1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dimensions = [\n",
    "    {\n",
    "        \"table\": f\"{catalog}.{target_schema}.DimPassengers\",\n",
    "        \"alias\": \"DimPassengers\",\n",
    "        \"join_keys\": [(\"passenger_id\", \"passenger_id\")] # fact_col, dim_col\n",
    "    },\n",
    "    {\n",
    "        \"table\": f\"{catalog}.{target_schema}.DimFlights\",\n",
    "        \"alias\": \"DimFlights\",\n",
    "        \"join_keys\": [(\"flight_id\", \"flight_id\")]\n",
    "    },\n",
    "    {\n",
    "        \"table\": f\"{catalog}.{target_schema}.DimAirports\",\n",
    "        \"alias\": \"DimAirports\",\n",
    "        \"join_keys\": [(\"airport_id\", \"airport_id\")]\n",
    "    },\n",
    "]\n",
    "\n",
    "# columns you want to keep from fact table (besides the surrogate keys)\n",
    "fact_columns = [\"amount\", \"booking_date\", \"modifiedDate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8474b046-e2ec-4d62-b96c-ec0fa4e07982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Last load date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91ac204-4379-4a96-9c5c-6c237b1f846e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# no back dated refresh\n",
    "if len(backdated_refresh) == 0:\n",
    "    # if table exists in destination\n",
    "    if spark.catalog.tableExists(f\"{target_schema}.{target_object}\"):\n",
    "        last_load = spark.sql(f\"select max({cdc_col}) from {catalog}.{target_schema}.{target_object}\").collect()[0][0]\n",
    "    else:\n",
    "        last_load = \"1900-01-01 00:00:00\" # initial load, so we can load everything\n",
    "# yes back dated refresh\n",
    "else:\n",
    "    last_load = backdated_refresh\n",
    "\n",
    "# test the last load\n",
    "last_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d78bc3a-9d95-456c-a1dc-a6f64a9ce73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dynamic fact query [bring keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1afd7f63-b88e-4d08-b2cc-da5ba8084493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_fact_query_incremental(fact_table, dimensions, fact_columns, cdc_column, processing_date):\n",
    "    fact_alias = \"f\"\n",
    "\n",
    "    # base columns to select\n",
    "    select_cols = [f'{fact_alias}.{col}' for col in fact_columns]\n",
    "\n",
    "    # build joins dynamically\n",
    "    join_clauses = []\n",
    "\n",
    "    for dim in dimensions:\n",
    "        table_full = dim[\"table\"]\n",
    "        alias = dim[\"alias\"]\n",
    "        table_name = table_full.split(\".\")[-1]\n",
    "        surrogate_key = f\"{alias}.{table_name}Key\"\n",
    "        select_cols.append(surrogate_key)\n",
    "\n",
    "        # build on clause\n",
    "        on_conditions = [\n",
    "            f\"{fact_alias}.{fk} = {alias}.{dk}\" for fk, dk in dim[\"join_keys\"]\n",
    "        ]\n",
    "        join_clause = f\"left join {table_full} {alias} on \" + \" and \".join(on_conditions)\n",
    "        join_clauses.append(join_clause)\n",
    "\n",
    "    # final select and join clauses\n",
    "    select_clause = \",\\n\".join(select_cols)\n",
    "    joins = \"\\n\".join(join_clauses)\n",
    "\n",
    "    # where clause for incremental filtering\n",
    "    where_clause = f\"{fact_alias}.{cdc_column} >= date('{last_load}')\"  \n",
    "\n",
    "    # final query\n",
    "    query = f\"\"\"\n",
    "        select\n",
    "            {select_clause}\n",
    "        from\n",
    "            {fact_table} {fact_alias}\n",
    "            {joins}\n",
    "        where\n",
    "            {where_clause}\n",
    "    \"\"\".strip()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288a729d-b4b8-4258-beb9-94f785354a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# showing query\n",
    "query = generate_fact_query_incremental(fact_table, dimensions, fact_columns, cdc_column, last_load)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb7fb7c-c6c0-4dd0-9e1c-b5d6405712e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### df_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae283cd-805e-4688-9104-7cc5d1b00dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fact = spark.sql(query)\n",
    "display(df_fact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf90ef5-1282-4474-a869-9fa0150b658c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865cf0f9-841c-4a5a-8217-05a1d2f30202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fact key columns merge condition\n",
    "fact_key_cols_str = \" and \".join([f\"src.{col} = tgt.{col}\" for col in fact_key_cols])\n",
    "fact_key_cols_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b26fcb1-2130-4573-8b55-7f30e6cb3976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "if spark.catalog.tableExists(f\"{catalog}.{target_schema}.{target_object}\"):\n",
    "    dlt_obj = DeltaTable.forName(spark, f\"{catalog}.{target_schema}.{target_object}\")\n",
    "    \n",
    "    dlt_obj.alias(\"tgt\").merge(df_fact.alias(\"src\"), fact_key_cols_str) \\\n",
    "        .whenMatchedUpdateAll(condition = f\"src.{cdc_column} >= tgt.{cdc_column}\") \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "else:\n",
    "    df_fact.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(f\"{catalog}.{target_schema}.{target_object}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3688501-d017-4298-89ea-6c4e927a1aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from flightsproject.gold.factbookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa01a6c5-4919-453f-9333-6e551276db5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# checking duplicates\n",
    "df = spark.sql(\"select * from flightsproject.gold.dimairports\").groupBy(\"DimAirportsKey\").count().filter(col(\"count\") > 1).display()\n",
    "\n",
    "df = spark.sql(\"select * from flightsproject.gold.dimflights\").groupBy(\"DimFlightsKey\").count().filter(col(\"count\") > 1).display()\n",
    "\n",
    "df = spark.sql(\"select * from flightsproject.gold.dimpassengers\").groupBy(\"DimPassengersKey\").count().filter(col(\"count\") > 1).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8220032807531988,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "6.GoldLayer_Facts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
